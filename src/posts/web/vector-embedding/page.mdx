---
title: ë²¡í„° ì„ë² ë”©(í…ŒìŠ¤íŠ¸)
date: 2024/12/16
description: í…ŒìŠ¤íŠ¸ ì„¤ëª…
tags: ['í…ŒìŠ¤íŠ¸']
author: ì´ì§„ìš©
---


`ì½”ë“œ`

~~ì·¨ì†Œ~~

<u>ì–¸ë”ë¼ì¸</u>

*ê¸°ìš¸ì–´ì§*

**ë³¼ë“œ**

1. ë¦¬ìŠ¤íŠ¸1

2. ë¦¬ìŠ¤íŠ¸2

3. ë¦¬ìŠ¤íŠ¸3

- ë¶ˆë¦¿1

- ë¶ˆë¦¿1

> ğŸ’¡ ì½œì•„ì›ƒ

> ì¸ìš©

[DIVIDER BLOCK NOT SUPPORTED]

[ ] ì²´í¬ë°•ìŠ¤

### ë…¼ë¬¸ ì´ˆë¡ ë²¡í„°í™”

```python
from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ë…¼ë¬¸ ì´ˆë¡ í…ìŠ¤íŠ¸ ìƒ˜í”Œ
abstracts = [
    "Deep learning architectures and algorithms have dramatically improved the performance of artificial intelligence.",
    "Natural language processing empowers computational systems to understand human language.",
    "Quantum computing is a new paradigm that exploits the principles of quantum mechanics.",
    "Climate change models are essential for understanding future environmental conditions.",
    ...
]

# ë…¼ë¬¸ ì´ˆë¡ì„ ì„ë² ë”©
def embed_texts(texts):
    encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    input_ids = encoded_inputs['input_ids'].to(device)
    attention_mask = encoded_inputs['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
    
    embeddings = outputs.last_hidden_state.mean(dim=1)  # ë¬¸ì¥ì˜ í‰ê·  ì„ë² ë”©
    return embeddings.cpu().numpy()  # GPUì—ì„œ CPUë¡œ ë°ì´í„° ì´ë™

# ëª¨ë“  ë…¼ë¬¸ ì„ë² ë”©
abstract_embeddings = embed_texts(abstracts)
```

1. ë¬¸ì¥ tokenize 

2. ë‹¨ì–´ vectorí™” 

### ê²€ìƒ‰

```python
# ìƒˆë¡œìš´ ë¬¸ì¥ ì„ë² ë”©
new_sentence = "Autonomous drone navigation through AI"
new_embedding = embed_texts([new_sentence])

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
similarity_scores = cosine_similarity(new_embedding, abstract_embeddings)

# ê°€ì¥ ìœ ì‚¬í•œ ë…¼ë¬¸ì˜ ì¸ë±ìŠ¤ë¥¼ ìƒìœ„ 3ê°œê¹Œì§€ ì°¾ê¸°
most_similar_indices = np.argsort(similarity_scores[0])[::-1][:3]

# ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ 3ê°œì˜ ë…¼ë¬¸ ì¶œë ¥
print("Top 3 most similar documents by BERT/cosine similarity:")
for index in most_similar_indices:
    print(abstracts[index])

```

### ê²°ê³¼

```plain text
Top 3 most similar documents by BERT/cosine similarity:
Automated systems for monitoring agricultural fields using drone-captured images.
Enhanced surveillance systems using AI-based object tracking.
Deep reinforcement learning for autonomous drone navigation.
```

![](./Pasted_image_20241211153241.png)